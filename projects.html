<!DOCTYPE HTML>

<html>
	<head>
		<title>Projects</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
            <header id="header">
                <h1><a href="index.html">Jiawei</a></h1>
                <nav id="nav">
                    <ul>
                        <li><a href="index.html">Home</a></li>
                        <li><a href="publications.html">Publications</a></li>
                        <li><a href="projects.html" class="button" target="_blank">Projects</a></li>
                        <li><a href="talks.html">Talks</a></li>
                        <li><a href="teaching.html">Teaching</a></li>
                        <li><a href="activities.html">Activities</a></li>
                    </ul>
                </nav>
            </header>


			<!-- Main -->
            <section id="main" class="container">
                <header>
                    <h2>Projects</h2>
                </header>

                <div class="row">
                    <div class="col-12">
                        <div id="filter-container">
                            <h3>Filters</h3>
                            <div class="filter-row">
                                <span class="filter-label">Tags:</span>
                                <div id="filter-tags"></div>
                            </div>
                        </div>

                        <section class="box" data-tags="#HCI, #Multimodal AI">
                            <h4>GANLAI: A Multimodal AI-Driven Virtual Reality Boxing Game Prototype for Stress Relief</h4>
                            <p>Yen-Ju Wang, Jia-Syuan Xiao, Yu-Ching Ger, Ting-Shan Pan, <b>Jia-Wei Liao</b>, Chen-Shi Liu, Yun-Chi Chen, Po-Feng Lin, OpenHCI Workshop, 2025 <i>(Accepted to TAAI 2025)</i>.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/苦Gym甘來_slides.pdf" class="custom-button" target="_blank">Slides</a></li>
                                <li><a href="projects/苦Gym甘來_poster.pdf" class="custom-button" target="_blank">Poster</a></li>
                                <li><a href="projects/苦Gym甘來_paper.pdf" class="custom-button" target="_blank">Extended Abstract</a></li>
                            </div>
                            GANLAI is a multimodal AI-driven VR boxing exergame that transforms exercise and emotional data into personalized narrative feedback for stress relief and reflection. By visualizing stress as in-game opponents and generating post-game encouragement and insights, it bridges immersive gameplay with real-world emotional processing. The system aims to extend psychological benefits beyond gameplay, fostering sustained motivation, self-awareness, and resilience.
                        </section>

                        <section class="box" data-tags="#Diffusion Model, #Music Generation">
                            <h4>DiffMusic: A Zero-shot Diffusion-Based Framework for Music Inverse Problem</h4>
                            <p><b>Jia-Wei Liao</b>, Pin-Chi Pan, and Sheng-Ping Yang, Deep Learning for Music Analysis and Generation Final Project, 2024.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/diffmusic.pdf" class="custom-button" target="_blank">Slides</a></li>
                                <li><a href="https://github.com/jwliao1209/DiffMusic" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#NLP, #LLM">
                            <h4>Taiwan-LLM Tutor: Revolutionizing Taiwanese Secondary Education with Large Language Model</h4>
                            <p><b>Jia-Wei Liao</b>, Ji-Jia Wu, Kun-Hsiang Lin, and Kang-Yang Huang, Applied Deep Learning Final Project, 2023.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/twllm_tutor_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="projects/twllm_tutor_slides.pdf" class="custom-button" target="_blank">Slides</a></li>
                                <li><a href="https://github.com/jwliao1209/TWLLM-Tutor" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Machine Learning, #Feature Engineering, #Multimodal AI">
                            <h4>Multimodal Pathological Voice Classification</h4>
                            <p>Chun-Hsien Chen, Shu-Cheng Zheng, <b>Jia-Wei Liao</b>, and Yi-Cheng Hung, AICUP, 2023</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/multimodal_pathological_voice_classification_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Multimodal-Pathological-Voice-Classification" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Data Analysis, #Business Analysis">
                            <h4>Optimizing Marketing Strategies and Data Analysis for Bag Brand</h4>
                            <p>Yi-Jhen Ciou, I-Ting Wang, Kai Yu, and <b>Jia-Wei Liao</b>, TMBA Final Project, 2023</p>
                            <div class="tags-container"></div>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision">
                            <h4>A Small Object Detection Framework for Unmanned Aerial Vehicles Images</h4>
                            <p>Jing-En Huang and <b>Jia-Wei Liao</b>, AI CUP, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/a_small_object_detection_framework_for_uav_images_report.pdf" class="custom-button" target="_blank">Report</a></li>
                            </div>

                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>We developed a state-of-the-art one-stage detection model as the baseline framework for the competition, with the goal of accurately detecting small objects. As a result, we achieved a top 5 ranking in the competition with a public score of 0.7394 and a private score of 0.7550.</p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/a_small_object_detection_framework_for_uav_images_image.jpeg" alt="" style="height: 200px;"/>
                                </div>
                            </div>

                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision">
                            <h4>Crop Image Recognition</h4>
                            <p><b>Jia-Wei Liao</b>, Yen-Jia Chen, Yi-Cheng Hung, Jing-En Hung, and Shang-Yen Lee, AI CUP, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/crop_image_recognition_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Crop-Image-Recognition" class="custom-button" target="_blank">Code</a></li>
                            </div>

                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>
                                        We trained EfficientNet and Swin Transformer model, incorporating auto-augmentation techniques to enhance dataset diversity. To assess the stability and benefits of ensemble methods, we applied Test-Time Augmentation (TTA) and ensemble strategies to both public and private datasets. This approach resulted in a public ranking of 9th and a private ranking of 8th, with scores of 0.9329 and 0.9344, respectively.										</p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/crop_image_recognition_image.png" alt="" style="height: 250px;"/>
                                </div>
                            </div>
                        </section>

                        <section class="box" data-tags="#NLP">
                            <h4>Explainable Information Tagging for Natural Language Understanding</h4>
                            <p><b>Jia-Wei Liao</b>, Jung-Mei Chu, and Chia-Chi Huang, AI CUP, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/explainable _information_tagging_for_natural_language_understanding_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Explainable-NLP" class="custom-button" target="_blank">Code</a></li>
                            </div>
                            <p>We constructed an explainable deep learning model for a NLP task by converting it into a summary task. We applied the T5 model to two datasets with different pre-processing methods and surpassed the original data's baseline. We achieved a public score of 0.801772 and a private score of 0.85190.</p>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision, #Medical AI">
                            <h4>Contour Segmentation for Spread Through Air Spaces in Lung Adenocarcinoma</h4>
                            <p><b>Jia-Wei Liao</b>, Kuok-Tong Ng, and Yi-Cheng Hung, AI CUP, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/aicup_2022_stas_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/STAS-Segmentation" class="custom-button" target="_blank">Code</a></li>
                            </div>

                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>
                                        We developed a UNet-based model with a diverse backbone, addressing the imbalance of foreground and background through a combination of DiceLoss and Focal Loss, and enhancing robustness through auto-augmentation. Our post-processing strategies improved accuracy, resulting in a 3rd place public ranking and 16th place private ranking, with scores of 0.9194 and 0.9109 respectively.
                                    </p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/aicup_2022_stas_image.jpg" alt="" style="height: 160px;"/>
                                </div>
                            </div>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision">
                            <h4>Supervised Learning for Few-Shot Orchid types Classification with Prior Guided Feature</h4>
                            <p>Yu-Hsi Chen, <b>Jia-Wei Liao</b>, and Kuok-Tong Ng, AI CUP, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/supervised_learning_for_few_shot_orchid_types_classification_with_prior_guided_feature_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Orchid-Classification" class="custom-button" target="_blank">Code</a></li>
                            </div>

                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>We developed a few-shot learning approach to accurately distinguish visually similar orchid species, addressing challenges posed by biotechnology-created species and limited training data. Using various models, data augmentations, and optimization techniques, we ensembled top predictions, achieving a Macro-F1 score of 0.9115 (public) and 0.8096 (private), ranking 15th out of 743 teams.</p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/supervised_learning_for_few_shot_orchid_types_classification_with_prior_guided_feature_image.png" alt="" style="height: 160px;"/>
                                </div>
                            </div>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision">
                            <h4>MediaTek Low-power Segmentation Competition</h4>
                            <p><b>Jia-Wei Liao</b>, Machine Learning Final Project, 2022.</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="https://github.com/jwliao1209/Low-Power-Segmentation" class="custom-button" target="_blank">Code</a></li>
                            </div>
                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>
                                        We proposed a lightweight semantic segmentation model optimized for embedded systems and traffic scene analysis in Asian regions, including Taiwan. The model balances accuracy, low power consumption, and real-time performance, designed for MediaTek's Dimensity Series platform. Evaluated using mIoU, a standard metric for semantic segmentation, it achieved a public test score of 0.5624.
                                    </p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/low_power_segmentation_image.png" alt="" style="height: 200px;"/>
                                </div>
                            </div>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision, #Medical AI">
                            <h4>Ultrasound Nerve Segmentation</h4>
                            <p><b>Jia-Wei Liao</b>, Kuok-Tong Ng, and Yi-Cheng Hung, VRDL Final Project (Kaggle), 2021</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/ultrasound_nerve_segmentation_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Ultrasound-Nerve-Segmentation" class="custom-button" target="_blank">Code</a></li>
                            </div>
                            <div style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center;">
                                    <p>
                                        We propose a deep learning method for ultrasound nerve segmentation using UNet with an EfficientNet backbone. Two novel techniques, Erosion Mask Smoothing (ELS) and adaptive Single Model Ensemble (ASME), improved performance. Evaluated with the Dice Similarity Coefficient (DSC), our approach achieved a private test score of 0.7234, surpassing the baseline.
                                    </p>
                                </div>
                                <div style="flex: 2; display: flex; justify-content: center;">
                                    <img src="images/ultrasound_nerve_segmentation_image.png" alt="" style="height: 180px;"/>
                                </div>
                            </div>
                        </section>

                        <section class="box" data-tags="#Machine Learning, #Computational Mathematics">
                            <h4>Low Rank Matrix Factorization for Recommender System</h4>
                            <p><b>Jia-Wei Liao</b>, Kuok-Tong Ng, and Yi-Cheng Hung, Introduction to Scientific Computing Final Project, 2021</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/low_rank_matrix_factorization_for_recommender_system.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/softImpute-ALS" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Text Mining">
                            <h4>Sentiment Analysis of Food Reviews on Yelp Platform</h4>
                            <p><b>Jia-Wei Liao</b>, Yi-Cheng Hung, and Yu-Lin Tsai, Introduction to Data Science Final Project, 2021</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/sentiment_analysis_of_reviews_on_yelp_platform.pdf" class="custom-button" target="_blank">Report</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Deep Learning, #Computational Mathematics">
                            <h4>Solving the Biharmonic Equation by Deep Neural Network</h4>
                            <p><b>Jia-Wei Liao</b>, Yu-Hsi Chen, and Woan-Rong Huang, Numerical Partial Differential Equations Final Project, 2021</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/solving_the_biharmonic_equation_by_deep_neural_network.pdf" class="custom-button" target="_blank">Slides</a></li>
                                <li><a href="https://github.com/jwliao1209/3D-Animation-Morphing" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Computer Graph, #Computational Mathematics">
                            <h4>3D Shape Morphing Animation based on Poisson Image Editing</h4>
                            <p><b>Jia-Wei Liao</b> and Kuok-Tong Ng, 3D Computationl Geometry Final Project, 2021</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="https://github.com/jwliao1209/3D-Animation-Morphing" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Image Processing, #Computational Mathematics">
                            <h4>Mean Curvature Flow on Graphs for Image Denoising</h4>
                            <p><b>Jia-Wei Liao</b> and Kuok-Tong Ng, Image Processing with Partial Differential Equations Final Project, 2020</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/on_the_mean_curvature_flow_on_graphs_with _applications_in_image_and_manifold_processing.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Mean-Curvature-Flow" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Deep Learning for Computer Vision, #Computational Mathematics">
                            <h4>Online Dictionary Learning for Image Inpainting</h4>
                            <p><b>Jia-Wei Liao</b> and Kuok-Tong Ng, Optimization for Data Science Final Project, 2020</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/online_dictionary_learning_for_image_inpainting_slides.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="https://github.com/jwliao1209/Online-Dictionary-Learning" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>

                        <section class="box" data-tags="#Image Processing">
                            <h4>Variational Models and Numerical Methods for Image Processing</h4>
                            <p><b>Jia-Wei Liao</b>, Chun-Hsien Chen, and Chen-Yang Dai, NCTS-USRP Final Project, 2020</p>
                            <div class="tags-container"></div>
                            <div class="button-container">
                                <li><a href="projects/2020_ncts_usrp_report.pdf" class="custom-button" target="_blank">Report</a></li>
                                <li><a href="projects/2020_ncts_usrp_slides.pdf" class="custom-button" target="_blank">Slides</a></li>
                                <li><a href="https://github.com/jwliao1209/Total-Variational-Model" class="custom-button" target="_blank">Code</a></li>
                            </div>
                        </section>
    
                    </div>
                </div>
            </section>

        <!-- Footer -->
            <footer id="footer">
                <ul class="copyright">
                    <li>&copy; 2026 Jiawei. All rights reserved.</li>
                </ul>
            </footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

			<script>
				document.querySelectorAll(".box").forEach(box => {
					const tagsContainer = box.querySelector(".tags-container");
					const tags = box.getAttribute("data-tags").split(", ");
					tags.forEach(tag => {
						const span = document.createElement("span");
						span.className = "tags";
						span.textContent = tag;
						tagsContainer.appendChild(span);
					});
				});

				const uniqueTags = new Set();
				document.querySelectorAll(".box").forEach(box => {
					const tags = box.getAttribute("data-tags").split(", ");
					tags.forEach(tag => uniqueTags.add(tag));
				});
		
				const filterTagsContainer = document.getElementById("filter-tags");
				uniqueTags.forEach(tag => {
					const tagButton = document.createElement("span");
					tagButton.className = "filter-tags";
					tagButton.textContent = tag.replace(/^#/, "");
					tagButton.setAttribute("data-tag", tag);
					filterTagsContainer.appendChild(tagButton);
				});

				const selectedTags = new Set();
				document.addEventListener("click", function (e) {
					if (e.target.classList.contains("filter-tags")) {
						const tag = e.target.getAttribute("data-tag");

						if (selectedTags.has(tag)) {
							selectedTags.delete(tag);
							e.target.classList.remove("selected");
						} else {
							selectedTags.add(tag);
							e.target.classList.add("selected");
						}
						filterPublications();
					}
				});

				function filterPublications() {
					const publications = document.querySelectorAll(".box");
					publications.forEach(publication => {
						const tags = publication.getAttribute("data-tags").split(", ");
						if (selectedTags.size === 0) {
							publication.style.display = "block";
						} else {
							const match = Array.from(selectedTags).some(tag => tags.includes(tag));
							publication.style.display = match ? "block" : "none";
						}
					});
				}
			</script>
	</body>
</html>
